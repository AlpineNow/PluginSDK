/**
 * COPYRIGHT (C) 2015 Alpine Data Labs Inc. All Rights Reserved.
 */

package com.alpine.plugin.core.spark.utils

import org.apache.spark.sql.{SQLContext, DataFrame}

import org.apache.spark.SparkContext
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.StructField

import com.databricks.spark.csv._

import com.alpine.plugin.core.annotation.AlpineSdkApi
import com.alpine.plugin.core.io._

/**
 * :: AlpineSdkApi ::
 */
@AlpineSdkApi
class BasicSparkUtils(
    sc: SparkContext) {
  def convertColumnTypeToSparkSQLDataType(
    columnType: ColumnType.ColumnType
  ): DataType = {
    columnType match {
      case ColumnType.Int => IntegerType
      case ColumnType.Long => LongType
      case ColumnType.Float => FloatType
      case ColumnType.Double => DoubleType
      case ColumnType.String => StringType
      case ColumnType.DateTime => TimestampType
      case _ =>
        throw new UnsupportedOperationException(columnType.toString + " is not supported.")
    }
  }

  def convertSchemaOutlineToSparkSQLSchema(
    schemaOutline: TabularSchemaOutline
  ): StructType = {
    StructType(
      schemaOutline.getFixedColumns().map{
        columnDef =>
          StructField(
            columnDef.columnName,
            convertColumnTypeToSparkSQLDataType(columnDef.columnType),
            true
          )
      }
    )
  }
}
