/**
 * COPYRIGHT (C) 2015 Alpine Data Labs Inc. All Rights Reserved.
 * Provides a templated base for Spark based plugins that take DataFrames.
 */

package com.alpine.plugin.core

import collection.mutable

import org.apache.spark.SparkContext
import org.apache.spark.sql.{SQLContext, DataFrame}

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

import com.alpine.plugin.core._
import com.alpine.plugin.core.io._
import com.alpine.plugin.core.datasource.OperatorDataSourceManager
import com.alpine.plugin.core.visualization.{VisualModel, VisualModelFactory}
import com.alpine.plugin.core.spark.{SparkRuntimeWithIOTypedJob, SparkIOTypedPluginJob}
import com.alpine.plugin.core.spark.utils.SparkUtils
import com.alpine.plugin.core.dialog.OperatorDialog

/**
 * Templated base for Spark plugin jobs operating on DataFrames.
 * Most jobs will want to use SparkDataFrameRuntime which takes
 * and returns Spark DataFrames.
 */
trait TemplatedSparkDataFrameJob[ReturnType, StorageType] {
  override def onExecution(sparkContext: SparkContext,
    appConf: mutable.Map[String, String],
    input: HdfsTabularDataset,
    operatorParameters: OperatorParameters,
    listener: OperatorListener,
    ioFactory: IOFactory): StorageType = {
    val sparkUtils = new SparkUtils(
      sparkContext,
      ioFactory
    )
    val dataFrame = sparkUtils.getDataFrame(input)
    listener.notifyMessage("Starting transformation")
    val results = transform(dataFrame)
    val storageFormat = operatorParameters.getStringValue("storageFormat")
    val outputPath = operatorParameters.getStringValue("outputPath")
    listener.notifyMessage(s"Saving results to ${path} in ${storageFormat}")
    saveResults(results, sparkUtils, storageFormat, path)
  }

  def transform(dataFrame: DataFrame): ReturnType
  def saveResults(results: ReturnType,
    storageFormat: String,
    path: String): StorageType
}

object DataFrameStorageFormat extends Enumeration {
  type DataFrameStorageFormat = Value
  val Parquet, Avro, TSV = Value
}

/**
 * Job base for Spark plugin jobs taking and returning DataFrames.
 */
trait SparkDataFrameJob extends TemplatedSparkDataFrameJob[DataFrame, HdfsTabularDataset] {

  /**
   * Save the results to the target path.
   */
  override def saveresults(transformedDataFrame: DataFrame, sparkUtils: SparkUtils,
    storageFormat: String, outputPath: String): HdfsTabularDataset = {
    DataFrameStorageFormat.withName(storageFormat) match {
      case Parquet => sparkUtils.saveAsParquet(outputPath, transformedDataFrame)
      case Avro => sparkUtils.saveAsAvro(outputPath, transformedDataFrame)
      case TSV => sparkUtils.saveAsTsv(outputPath, transformedDataFrame)
    }
  }
}

/**
 * Control the GUI of your Spark job, through this you can specify
 * any visualization for the output of your job, and what params
 * the user will need to specify.
 */
trait TemplatedSparkDataFrameGUINode[StorageType] extends OperatorGUINode[Tuple2[HdfsRawTextDataset, HdfsBinaryFile], StorageType] {
}

/**
 * Control the GUI of your Spark job, through this you can specify
 * any visualization for the output of your job, and what params
 * the user will need to specify. Uses the provided operator to generate
 * an updated schema.
 */
abstract class SparkDataFrameGUINode(operator: SparkDataFrameRunTime) extends TemplatedSparkDataFrameGUINode[HdfsDelimitedTabularDataset]() {

  lazy val sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())
  lazy val sparkUtils = new SparkUtils(SparkContext.getOrCreate(), new IOFactoryImpl())

  /**
   * Defines the params the user will be able to select. The default
   * asks for desired output format & output location.
   */
  override def onPlacement(
    operatorDialog: OperatorDialog,
    operatorDataSourceManager: OperatorDataSourceManager,
    operatorSchemaManager: OperatorSchemaManager): Unit = {

    val formats = DataFrameStorageFormat.values.map(_.toString())
    operatorDialog.addDropdownBox(
      "storageFormat",
      "Storage format",
      formats,
      formats.head
    )

    operatorDialog.addHdfsFileSelector(
      "outputPath",
      "Output Path",
      "/tmp",
      operatorDataSourceManager
    )
  }

  /**
   * Updates the output schema based the provided operator.
   * An empty Dataframe with the correct schema is fed in and the resulting
   * dataframe's schema is transfered back to an Alpine schema.
   * @group internals
   */
  private def updateOutputSchema(
    inputSchemas: mutable.Map[String, TabularSchemaOutline],
    params: OperatorParameters,
    operatorSchemaManager: OperatorSchemaManager): Unit = {
    // There can only be one input schema.
    assert(inputSchemas.size == 1)
    // Get the Alpine schema and construct a matching empty DataFrame
    val inputSchema = inputSchemas.head._2
    val sparkSchema = sparkUtils.convertSchemaOutlineToSparkSQLSchema(inputSchema)
    val dataFrame = sqlContext.createDataFrame(sparkContext.emptyRDD[Row], sparkSchema)
    // Apply the transformation and extract our resulting schema
    val outputSparkSchema = e.transform(dataFrame).schema
    val outputSchema = sparkUtils.convertSparkSQLSchemaToSchemaOutline(outputSparkSchema)
    operatorSchemaManager.setOutputSchemaOutline(outputSchema)
  }
  /**
   * Calls update schema when the parameters are changed
   * @group internals
   */
  override def onInputOrParameterChange(
    inputSchemas: mutable.Map[String, TabularSchemaOutline],
    params: OperatorParameters,
    operatorSchemaManager: OperatorSchemaManager): Unit = {
    this.updateOutputSchema(
      inputSchemas,
      params,
      operatorSchemaManager
    )
  }
}
